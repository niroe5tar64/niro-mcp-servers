# 各章の要約

## 要旨

LLMエージェントの失敗の多くは推論能力ではなく問題表現の暗黙性に起因すると主張。Model-First Reasoning (MFR)を提案し、推論前にエンティティ・状態変数・行動・制約を明示化する二段階手順で制約違反と不整合を減らす。多様な制約計画課題でCoT/ReActより良好な結果を示す。

## 1 はじめに

LLMの推論能力は向上したが、長期計画や制約の厳しい課題で不整合が多い。原因は暗黙の問題表現にあるとし、明示的モデルを先に構築するMFRを導入する動機を示す。

## 2 背景と関連研究

CoTとReActは推論の手続きは改善するが、問題構造の明示化を欠くため長期の一貫性に弱い。古典的AIプランニングや認知科学のメンタルモデルは明示的表現を重視しており、MFRはそれらの考え方をLLMに適用する位置づけ。

## 3 Model-First Reasoning

MFRは「モデル構築→モデル上の推論」の二段階。フェーズ1でエンティティ・状態変数・行動・制約を明示し、フェーズ2でそれに従った計画を生成する。プロンプトだけで実装でき、暗黙仮定や状態漂流を抑制する。

## 4 実験設定

CoT/ReAct/MFRを、スケジューリング・経路計画・資源配分など制約駆動課題で比較。定性評価として制約充足、暗黙仮定、構造的明確性を指標にし、複数LLMで独立に実行。

## 5 結果と分析

MFRは制約違反と暗黙仮定が最も少なく、構造的明確性が高い。CoTは中間状態の欠落や仮定の導入が多く、ReActは改善するが長期で整合性が崩れる。失敗の主要因は表現の欠陥だと示唆。

## 6 議論

MFRは表現を外化することでソフトな記号的グラウンディングを提供し、制約の厳しい長期計画で有効。コスト増はあるが、検証可能性と信頼性向上の利点が大きい。モデル再利用でコスト低減の余地がある。

## 7 結論

MFRは明示的モデル化と推論の分離により、LLM計画の制約遵守と整合性を改善する。幻覚や計画失敗は推論ではなく表現の問題であり、明示的モデルは信頼できるエージェントの基盤になると結論づける。
