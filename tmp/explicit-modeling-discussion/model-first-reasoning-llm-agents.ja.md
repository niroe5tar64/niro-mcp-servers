# モデル・ファースト推論によるLLMエージェント: 明示的な問題モデル化で幻覚を低減する

Gaurav Kumar
Independent Researcher (Stanford AI Professional Program)

Annu Rana
Independent Researcher (IESE EMBA Program)

## 要旨

大規模言語モデル（LLM）は、Chain-of-Thought（CoT）や ReAct などのプロンプト戦略により、推論と計画で顕著な能力を示している。にもかかわらず、LLMベースのエージェントは複雑な多段の計画課題で失敗し続け、制約違反・状態追跡の不整合・些細な変化で崩れる脆い解を頻発させる。

我々は、これらの失敗の多くは推論そのものの欠陥ではなく、明示的な問題表現の欠如に起因すると主張する。人間の科学的推論、古典的AIプランニング、意思決定の認知モデルとは対照的に、現在のLLMプロンプトは暗黙かつ不安定な内部モデル上で推論を進めさせている。

これらに着想を得て、我々は Model-First Reasoning（MFR）を提案する。これは、推論や計画の前に、LLMに問題の構造化モデル（エンティティ、状態変数、前提と効果を備えた行動、制約）を明示的に構築させる二段階パラダイムである。推論はこの構築されたモデルに厳密に従って実行される。

多様な制約駆動の計画領域における実験により、MFRは制約違反を大幅に減らし、長期的整合性を改善し、CoTやReActよりも解を解釈・検証しやすくすることを示す。アブレーション研究は、モデル構築と推論の分離が改善に不可欠であることを確認した。

我々は、LLM計画における多くの失敗は推論ではなく表現に根本原因があり、明示的な問題モデル化は信頼できるエージェントAIシステムの基盤要素とみなされるべきだと結論づける。

arXiv:2512.14474v1 [cs.AI] 16 Dec 2025

## 1 はじめに

大規模言語モデル（LLM）は自然言語理解・推論・意思決定で優れた能力を示し、計画、問題解決、複雑環境との相互作用を行う自律エージェントとして利用できるようになった。Chain-of-Thought（CoT）や ReAct といったプロンプト戦略は、明示的な中間推論の生成や、推論と行動の交互実行を促すことで、多段推論を大きく改善した。それでも、LLMベースのエージェントは複雑で長期的な課題において制約違反、計画の不整合、脆い挙動を高い頻度で示す。

これらの失敗は、正しさが多段にわたる一貫した内部状態の維持、複数の相互作用する制約の尊重、暗黙の仮定の回避に依存する領域で顕著である。医療のスケジューリング、資源配分、手順実行など、正確性や安全性が重要な計画問題が該当する。現行のアプローチは主として推論過程そのものの改善に焦点を当ててきたが、我々はより根源的な制約を見落としていると主張する。すなわち、推論が明示的な問題表現なしに実行されていることだ。

### 1.1 LLMエージェントにおける暗黙的推論の限界

Chain-of-Thought は、最終回答の前に段階的説明を生成させることで推論精度を高める。しかし CoT は、解の妥当性を支配するエンティティ・状態変数・制約を明示的に定義させない。その結果、状態はモデルの潜在表現や自然言語出力に暗黙的に追跡され、推論が長くなるほど漂流・抜け・矛盾を起こしやすい。

ReAct スタイルのエージェントは推論と行動・観測を交互に行い、外部ツールや環境との相互作用を可能にする。これは適応性を高めるが、状態追跡は依然として自由形式テキストに分散したままである。観測は導出ではなく仮定されることが多く、制約は全体としてほとんど強制されない。そのため、長期では局所的に一貫して見える推論が、全体として不整合になることがある。

これらのアプローチは、推論手続きの改善だけで信頼できる計画が可能だという暗黙の前提を置いている。実際には、モデルが一貫した問題表現を推論過程で維持できると仮定しているが、その表現を明示・検証可能にする要求は課していない。

### 1.2 モデルに基づく推論としての推論

対照的に、人間の推論（科学、工学、日常の問題解決）は本質的にモデルベースである。科学は、関連エンティティ・変数・支配法則を定義してから推論を進める。工学では、最適化の前に明示的モデルを構築してシステム挙動を分析する。認知科学でも、人間の推論は内部のメンタルモデル上での推論・予測として理解されている。

推論の誤りは、推論規則の欠陥ではなく、不完全または誤ったモデルに起因することが多い。重要な変数や制約が抜け落ちると、論理的に妥当な推論でも誤った結論に至る。この観点では、信頼できる推論は「何が存在し、どう変化し、何が不変か」という明示的表現を前提とする。

古典的AIプランニングはこの原理を PDDL などの明示的ドメインモデルで形式化し、エンティティ、行動、前提条件、効果、制約を定義してから計画する。推論は固定された検証可能な構造上で行われる。一方、LLMエージェントは通常、モデル化と推論を単一の生成過程に折り畳み、基盤構造を暗黙かつ不安定にしてしまう。

この視点から見ると、幻覚は単なる誤った記述の生成ではない。むしろ、明確な問題モデルのない推論の症状である。

### 1.3 Model-First Reasoning

これらの観察に基づき、我々は Model-First Reasoning（MFR）を提案する。これは、LLMエージェントにおいて問題表現と推論を明示的に分離するパラダイムである。MFRでは、解や計画を生成する前に、モデルが明示的な問題モデルを構築する。モデルには以下が含まれる。

- 関連エンティティ
- 状態変数
- 前提条件と効果を備えた行動
- 妥当な解を定義する制約

モデル構築が完了してから、LLMは推論・計画フェーズに入り、定義したモデルの範囲内でのみ解を生成する。この分離は表現の足場を導入し、暗黙の潜在状態追跡への依存を減らし、無言の仮定の導入を抑制する。

重要な点として、MFRはアーキテクチャ変更、外部の記号ソルバー、追加学習を必要としない。プロンプトのみで実装可能であり、既存のLLMやエージェントフレームワークにすぐ適用できる。

### 1.4 貢献

本論文の貢献は以下のとおりである。

- LLMの計画・推論課題における主な失敗原因が、暗黙で不安定な問題表現であることを指摘する。
- 推論の前に明示的な問題モデル構築を要求する二段階パラダイム、Model-First Reasoningを提案する。
- 多様な制約駆動計画ドメインで、MFRが制約順守・整合性・解の質を向上させることを実証する。
- 幻覚や計画エラーを推論能力の不足ではなく表現の失敗として捉え直す概念的分析を提示する。

## 2 背景と関連研究

本節では、MFRをLLM推論、エージェントアーキテクチャ、古典的プランニングの関連研究に位置づける。既存手法は推論手続きを改善する一方で、問題表現の明示化を軽視していることを指摘する。

### 2.1 Chain-of-Thought推論

Chain-of-Thought（CoT）プロンプトは、最終回答前に中間推論を生成させることで性能を向上させる。算術、常識推論、記号課題で顕著な改善が示され、潜在的推論過程を自然言語に外化する手法である。

しかしCoTは、解くべき問題構造の明示的定義を要求しない。エンティティ・状態変数・制約は暗黙的かつ動的に導入され、長期または制約の重い課題では、局所的一貫性は保てても全体的整合性が崩れる。制約違反、暗黙の仮定、状態遷移の欠落が典型的な失敗である。

表現の観点では、CoTは「どう推論するか」を改善しても「何に対して推論するか」を改善しない。

### 2.2 ReActとエージェント推論

ReAct は推論と行動・観測を交互に行い、ツール・環境・外部APIとの相互作用を可能にする。多くのエージェントフレームワークの基盤となり、対話的設定での適応性を改善した。

それでも ReAct は、自由形式テキストに分散した暗黙状態追跡に依存する。観測は導出ではなく仮定であり、制約は明示的に表現・検証されない。軌跡が長くなるほど状態整合性が劣化し、誤りが累積する。

ReAct は制御ループを導入したが、形式的な問題表現は導入しない。推論・行動・状態追跡は依然として単一の生成過程に絡み合っている。

### 2.3 古典的AIプランニングと明示モデル

LLMベースのアプローチと対照的に、古典的AIプランニングは問題定義と問題解決を明確に分離する。STRIPS や PDDL などの形式的枠組みは、以下の定義を要求する。

- オブジェクトとエンティティ
- 状態変数
- 前提条件と効果を持つ行動
- 目標条件と制約

計画アルゴリズムは固定モデル上で検索・検証し、正しさの保証を可能にする。これらの体系はLLMほどの柔軟性や汎用性はないが、信頼できる推論には安定した明示的表現が前提であることを示している。

MFRはこの伝統に着想を得つつ、モデル自体をLLMが自然言語または半構造形式で構築する点で根本的に異なる。

### 2.4 メンタルモデルと認知的視点

認知科学は、人間がメンタルモデルを構築して推論することを強調してきた。人は関連構造を捉える内部表現を作り、不要な詳細は省く。誤りは論理推論の失敗よりも、不完全・不正確なモデルに起因することが多い。

この視点は、推論が表現フレームワーク内で行われるという哲学的見解と一致する。推論は構造を作るのではなく、構造の上で行われる。構造が暗黙または不安定だと推論は不確かになる。

LLMは内部表現の外化を要求されることがほとんどない。MFRは推論前にモデルを記述させることでこのギャップを埋め、表現を検査可能・修正可能にする。

### 2.5 Model-First Reasoningの位置づけ

MFRは既存研究と以下の3点で異なる。

- モデリングと推論を明示的に分離し、交互化しない。
- 表現の失敗を推論エラーの主因として扱う。
- LLM自身に問題モデル構築を要求し、人手の形式化への依存を減らす。

MFRは記号プランナーのような厳格形式を課さず、CoTやReActのように構造を暗黙にしない。軽量なプロンプト手法として、柔軟性と安定性を両立する。

この位置づけにより、MFRは既存のLLMエージェントに統合可能であり、正確性・解釈性・制約遵守が重要な領域で特に有効となる。

## 3 Model-First Reasoning

MFRは、問題表現と問題解決を明示的に分離する問題解決パラダイムである。推論・計画・行動の前に、LLMが問題空間の明示的モデルを構築する。以降の推論はこのモデル内に制約される。

### 図1: 推論パラダイムの比較（CoT, ReAct, MFR）

- CoT: 問題記述 → 段階的推論 → 回答
- ReAct: 問題記述 → 推論 → 行動/観測 → 回答
- MFR: 問題記述 → 明示モデル（エンティティ、状態、行動、制約） → モデル上の推論・計画 → 構造化計画/検証可能解

### 3.1 概要

自然言語で与えられた問題記述に対し、MFRは二段階で進む。

1. モデル構築: エンティティ・状態変数・行動・制約を明示的に定義する。
2. 推論と計画: 先に構築したモデルのみを用いて解を生成する。

第二段階は第一段階の出力に条件づけられる。モデル構築は単なる中間推論ではなく、後続推論を拘束する表現上のコミットである。

### 3.2 フェーズ1: モデル構築

モデル構築フェーズでは、LLMは問題領域の理解を明示的に記述する。出力は以下の要素からなる構造化記述である。

- エンティティ: 問題に関与するオブジェクトやエージェント
- 状態変数: 時間とともに変化する属性
- 行動: 状態を変更する操作（前提条件と効果を含む）
- 制約: 常に守るべき不変条件や制限

モデルは自然言語、半構造テキスト、擬似形式記法でよい。固定形式は要求せず、柔軟性を優先する。重要なのは、表現が明示的・検査可能・安定であることだ。

このフェーズでは解を生成してはならず、表現と推論の分離を強制する。

### 3.3 フェーズ2: モデル上の推論

モデル構築後、LLMは解や計画を生成する。推論は明示モデルによって制約される。

- 行動は前提条件を満たす必要がある。
- 状態遷移は定義された効果と整合しなければならない。
- 制約は各ステップで常に満たされる。

モデルが外部化されているため、違反は可視化・診断可能になる。潜在状態に隠れていた誤りが、モデルと計画の不整合として表出する。

このフェーズは定義済み状態空間での古典的プランニングに似ているが、推論は記号プランナーではなく生成モデルにより行われる。

### 3.4 プロンプト構造

MFRはプロンプトのみで実装可能であり、一般的な二段テンプレートは以下の通り。

**フェーズ1（モデル構築）プロンプト**

以下の問題を分析せよ。まず、(1) 関連エンティティ、(2) 状態変数、(3) 前提条件と効果を持つ行動、(4) 制約 を列挙して問題モデルを明示的に定義せよ。まだ解を提案してはならない。

**フェーズ2（推論）プロンプト**

上で定義したモデルのみを用いて、段階的な解の計画を生成せよ。全ての行動が定義された制約と状態遷移に従うことを保証せよ。

単一プロンプト内で二段を実行することも、二つの連続プロンプトに分けることもできる。

### 3.5 なぜMFRは有効か

MFRはLLMの根本的制約（暗黙で不安定な内部表現）を直接扱う。構造の外化を強制することで、以下を実現する。

- 潜在状態追跡への依存を減らす
- 暗黙の仮定を防ぐ
- 長期的整合性を改善する
- 人間・自動双方の検証を可能にする

この視点では、MFRはソフトな記号的グラウンディングである。厳密な記号制約ではないが、複雑な計画課題で推論を安定化させるに足る構造を導入する。

### 3.6 既存パラダイムとの関係

MFRは既存手法を置き換えるのではなく、基盤層として補完する。

- 推論フェーズ内でCoTと併用可能。
- ReAct型エージェントでは、モデルを永続状態として統合可能。

MFRは、制約が厳しく安全性が求められる領域で、既存手法の頑健性を高める。

## 4 実験設定

### 4.1 目的

MFRが、CoTおよびReActと比較して、信頼性・制約順守・構造的明確性を向上させるかを評価する。代表的な計画課題での質的評価に重点を置く。

### 4.2 推論戦略

以下の三つの戦略を比較した。

- **Chain-of-Thought（CoT）**: 明示的モデルなしに段階的推論を促す。中間推論は生成されるが、エンティティや制約は暗黙のまま。
- **ReAct**: 推論と行動・観測を交互に行う。状態追跡は自然言語に依存し、制約の強制は限定的。
- **Model-First Reasoning（MFR）**: 先に問題モデルを構築し、定義したモデルのみを用いて推論する。構造的な接地と解釈性を高める。

### 4.3 タスク設計

制約のある計画課題を選定し、暗黙推論で誤りが生じやすく、明示モデルで改善が見込めるケースを対象とした。例:

- 多段の投薬スケジューリング
- 時間依存の経路計画
- 順序制約付き資源配分
- 論理パズル解法
- 手順合成タスク

### 4.4 プロンプトと実行

プロンプトは推論指示のみが異なるよう設計し、課題記述は同一に保った。MFRではモデル定義後に計画を生成する。CoTとReActは標準手順に従った。

複数のLLM（例: ChatGPT, Gemini, Claude）で独立に実行し、クロスコンタミネーションを避けた。各課題の代表例を著者が質的評価した。

### 4.5 評価基準

出力は以下の3観点で評価した。

1. 制約充足: 明示/暗黙の制約に従っているか
2. 暗黙の仮定: 未記述の行動や状態が導入されていないか
3. 構造的明確性: 解が解釈・検証可能な構造を持つか

評価は定性的であり（Low/Medium/High 等）、全例で一貫した基準を適用した。検証は手動で行い、出力と相互照合した。

### 4.6 実験設定の限界

- 選定例に基づく評価であり、網羅的ベンチマークではない。
- 定性評価は保守的だが、細かな差異を捉えきれない場合がある。
- モデル構築の正確性に依存し、誤ったモデルは計画品質に直結する。

## 5 結果と分析

### 5.1 概要

多段スケジューリング、経路計画、資源配分などの計画課題で、CoT・ReAct・MFRを比較した。制約順守、論理整合性、構造的明確性の定性評価を行い、課題条件に対する手動検証を実施した。

### 5.2 推論戦略の比較

**表1: 推論戦略の定性比較**

| 推論戦略 | 制約違反 | 暗黙の仮定 | 構造的明確性 |
|---|---|---|---|
| Chain-of-Thought (CoT) | 中 | 頻繁 | 低 |
| ReAct | 中〜低 | ときどき | 中 |
| Model-First (MFR) | 低 | 稀 | 高 |

**図2: 推論戦略の定性比較**

- 制約違反・暗黙の仮定は、MFRが最も低い。
- 構造的明確性は、MFRが最も高い。
- 定性レベルは Low=1, Medium=2, Medium-High=3, High=4 とし、Rare/Frequent/Occasional は 1/3/2 にマッピング。

### 5.3 Chain-of-Thoughtの分析

CoTは流暢な段階的推論を生み出すが、明示モデルがないため、以下が頻発した。

- 重要な中間状態の欠落
- 未記述の行動や仮定の導入
- ステップ間のグローバル整合性の崩れ

暗黙的な状態追跡のみへの依存が限界となる。

### 5.4 ReActの分析

ReActは推論と行動の交互化により局所推論を改善した。しかし、

- 観測が導出ではなく仮定になる場合がある
- グローバル制約が一貫して強制されない
- 長期で状態追跡が劣化する

という問題が残った。

### 5.5 Model-First Reasoningの分析

MFRの主要な利点は以下である。

- **制約の明示的接地**: モデルが安定した参照点となり、違反を減らす。
- **暗黙仮定の削減**: 明確なエンティティと行動定義により、欠落情報の補完を抑制する。
- **構造的明確性の向上**: 計画が解釈・検証しやすくなる。

### 5.6 解釈

これらは、LLMの失敗が推論能力よりも表現に起因するという仮説を支持する。モデル化と推論の分離により、問題構造を外部化し、潜在表現への依存を減らし、ソフトな記号的グラウンディングを実現する。制約が重い長期課題で特に有効である。

### 5.7 限界

- **課題範囲**: 構造化された制約駆動タスクで効果が顕著。
- **トークンコスト**: 明示モデルの構築により入出力が増大。
- **モデル依存**: モデル構築の精度が品質を左右する。
- **形式的検証ではない**: リスクは低減するが正当性を保証しない。

## 6 議論

実験結果は、LLMエージェントの推論失敗が推論規則ではなく問題表現の不備に起因するという仮説を裏付ける。MFRはモデル化と推論を分離し、解空間を制約する構造的足場を提供する。

主な観察は以下の通り。

- **表現の失敗**: CoTやReActは流暢でも、隠れた制約違反が生じ、幻覚は表現上の問題である可能性が高い。
- **ソフト記号的グラウンディング**: MFRのモデル構築は自然言語課題を構造化し、推論を安定化する。
- **タスク複雑性の影響**: 制約が多く長期の計画タスク（医療スケジューリング、資源配分など）で効果が顕著。
- **再現性と解釈性**: 明示モデルは検査・検証・デバッグを容易にし、信頼性を高める。

MFRは入出力コストを増やすが、正確性と検証可能性の向上がそのトレードオフを正当化する。今後は、繰り返しタスクでモデルを再利用し、モデル構築コストを償却することが望まれる。

## 7 結論

本論文は、問題モデル化と推論を分離する Model-First Reasoning（MFR）を提案した。複数の計画ドメインでの実験により以下を示した。

- 明示モデル構築により制約違反と暗黙の仮定が大幅に減少する。
- MFRはCoTやReActよりもグローバル整合性と解の質を改善する。
- モデル化と推論の分離は、LLM計画における表現的失敗を緩和する。

我々の知見は、幻覚や計画エラーを推論能力不足ではなく表現的失敗として捉え直す。明示的モデル化は、構造化された多段推論を行うAIエージェントにおいて信頼性・解釈性・検証性を高める基盤となる。

## 参考文献

[1] Richard Fikes and Nils Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 189–208, 1971.

[2] Philip N. Johnson-Laird. Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness. Harvard University Press, 1983.

[3] Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. PDDL - The Planning Domain Definition Language. Technical Report, AIPS-98 Planning Competition Committee, 1998.

[4] Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, and Roberto Pieraccini. Pre-act: Multi-step planning and reasoning improves acting in llm agents. arXiv preprint, 2025.

[5] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint, 2022.

[6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint, 2022.

[7] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint, 2022.
