# Multi-Agent 議論の効果と課題

本ドキュメントは、複数AI Agent会議の効果・課題・価値について整理したものである。

## 期待できる効果

### 1. 視点の網羅性向上

| 単純な壁打ち | 複数Agent会議 |
|-------------|--------------|
| 聞かれたことに答える | ロールが観点を強制的に分離 |
| 抜け漏れは質問者依存 | `risk-security`が見落としを拾う構造 |

例: 「この機能どう思う？」と聞くと良い点を中心に答えがちだが、`ops-cost`ロールがいれば運用負荷の懸念が出やすい。

### 2. 思考の深掘り促進

- 異なるロール間の「摩擦」が論点を浮き彫りにする
- `architect`の「拡張性重視」vs `ops-cost`の「シンプルさ重視」など
- 単純な壁打ちでは「どちらも大事」で終わりがち

### 3. 議論の構造化・再現性

- ワークフローが決まっているため、毎回同じ品質の検討ができる
- ログが残り、後から「なぜこの決定をしたか」を追跡可能
- 属人化しにくい

### 4. 認知負荷の分散

- 人間が全ての観点を同時に考える必要がない
- 「セキュリティはAIに任せて、自分はUXに集中」が可能

## 改善が難しい課題

### 1. 「同じ脳」問題

全ロールが同一のLLMから生成されるため、根本的なバイアスや知識の欠落は共有される。人間の多様なチームのような「本当の視点の違い」は生まれない。

例: 全員が同じ技術スタックしか知らない状態と同じ。

### 2. 表面的な対立になりやすい

- ロールを演じているだけで、本気で反論するインセンティブがない
- 「それも一理ありますね」で収束しがち
- 真剣な議論の緊張感が欠ける

### 3. コンテキスト希薄化

| 単純な壁打ち | 複数Agent会議 |
|-------------|--------------|
| 文脈が蓄積 | ロール切り替えで文脈が薄れる |
| 深い議論が可能 | 表層的な意見交換になりやすい |

### 4. オーバーヘッド

- 単純な問いに対しても「会議」を回すコスト
- 明らかな答えがある場合でも全ロールから意見を集める無駄
- 適切な使い分けの判断が難しい

### 5. 収束の難しさ

- facilitatorも同じLLMなので「どの意見を重視すべきか」の判断基準が曖昧
- 結局、人間が判断する必要がある部分は変わらない

## 効果が期待できるケース

| 効果あり | 効果薄い |
|---------|---------|
| 複雑な意思決定（アーキテクチャ選定） | 単純な実装方法の質問 |
| トレードオフの明確化 | 正解が1つの問題 |
| 抜け漏れチェック（レビュー的用途） | 探索的なブレスト |
| ドキュメント化が必要な決定 | カジュアルな相談 |

## それでも価値がある条件

### 1. 「チェックリストの代替」として使う場合

目的を「創造的な議論」ではなく「観点の網羅確認」に絞る。

- 人間が考えた案を、複数ロールに「レビュー」させる
- 「同じ脳」でも、強制的に異なる切り口で見るプロンプトには意味がある
- OWASP Top 10のように「チェック項目を擬人化した」と考える

### 2. 思考の外部化・記録が必要な場合

- 意思決定の根拠を後から説明する必要がある組織
- 「なぜこの選択をしたか」をログとして残したい
- 単純な壁打ちより構造化された記録が残る

### 3. 人間の認知バイアスを補正したい場合

人間が「この設計良さそう！」と思った時：
- 単純な壁打ち: 「いいですね」と同調しがち
- 複数Agent会議: `risk-security`が強制的に懸念を出す

ロールがあることで「反論する役割」が明示され、同調圧力を軽減できる。

## 結論

「複数Agent会議」は、人間のチームミーティングの代替ではなく、**構造化されたセルフレビュー手法**として位置づけるのが適切である。

この位置づけであれば価値がある。「会議」より「多角的レビュープロセス」と呼んだ方が期待値が合うかもしれない。

### 推奨アプローチ

1. 最初から完璧を期待しない
2. 効果があったケース／なかったケースを記録する
3. 「壁打ちで十分だった」と判断したら戻せばいい

実験として回してみて、実際の意思決定に役立つかを検証することを推奨する。
